# Fixed params
seed: 42
model_name: bert-base-uncased
base_tokenizer: bert-base-uncased
mlm_probability: 0.15
field_text: sentences
field_sic2: sic2
field_sic3: sic3
field_sic4: sic4

# Adjustable params
report_to: wandb
max_seq_len: 512
train_batch_size: 64
val_batch_size: 64
learning_rate: !!float 5e-5
num_train_epochs: 1
max_steps: 10000
num_warmup_steps: 100
num_workers: 4
weight_decay: 0.01
gradient_accumulation_steps: 2
grad_clip: 1.0
precision: bf16
logging_steps: 100
eval_strategy: steps
eval_steps: 500
save_total_limit: 3
save_strategy: steps
save_steps: 500
load_best_model_at_end: False
metric_for_best_model: "eval_loss"
greater_is_better: False
val_ratio: 0.1
wandb_mode: "online"
wandb_project: "pretrain-businessbert2"

# Loss weights
loss_weights:
  mlm: 1.0
  ic2: 1.0
  ic3: 0.8
  ic4: 0.5
  consistency: 0.2

# Data and Saving
jsonl_path: "${data}"
save_dir: ./outputs/pretrain
safe_serialization: false
save_safetensors: false
