# Pretraining configuration
seed: 42
model_name: bert-base-uncased
base_tokenizer: bert-base-uncased
max_seq_len: 128
train_batch_size: 256
val_batch_size: 256
learning_rate: !!float 5e-5
num_train_epochs: 1
max_steps: 1000000
num_warmup_steps: 10000
num_workers: 4
weight_decay: 0.01
gradient_accumulation_steps: 1
grad_clip: 1.0
logging_steps: 50
eval_strategy: steps
eval_steps: 500
save_strategy: steps
save_steps: 100000
val_ratio: 0.1
precision: bf16

# MLM params
mlm_probability: 0.15

# Loss weights
loss_weights:
  mlm: 1.0
  ic2: 1.0
  ic3: 0.8
  ic4: 0.5
  consistency: 0.2

# Progress
report_to: wandb

# Data
jsonl_path: "${data}"
field_text: sentences
field_sic2: sic2
field_sic3: sic3
field_sic4: sic4

# Saving
save_dir: ./outputs/pretrain
safe_serialization: false
