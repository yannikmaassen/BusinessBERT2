# Pretraining configuration
seed: 42
model_name: bert-base-uncased
base_tokenizer: bert-base-uncased
max_seq_len: 512
train_batch_size: 64
val_batch_size: 64
learning_rate: !!float 3e-5
adam_epsilon: !!float 1e-8
num_train_epochs: 10
num_warmup_steps: 500
num_training_steps: 1000
num_workers: 4
weight_decay: 0.01
gradient_accumulation_steps: 1
grad_clip: 1.0
fp16: false
logging_steps: 50
save_total_limit: 2
save_strategy: steps
save_steps: 500
evaluation_strategy: steps
evaluation_steps: 500
val_ratio: 0.1
precision: bf16

# Progress
report_to: wandb
disable_tqdm: false

# Data
jsonl_path: "${data}"
field_text: sentences
field_sic2: sic2
field_sic3: sic3
field_sic4: sic4

# Saving
save_dir: ./outputs/pretrain
safe_serialization: false

# MLM params
mlm_probability: 0.15
random_token_probability: 0.10
keep_token_probability: 0.10

# Loss weights
loss_weights:
  mlm: 1.0
  sop: 0.2
  ic2: 1.0
  ic3: 0.8
  ic4: 0.5
  consistency: 0.1
